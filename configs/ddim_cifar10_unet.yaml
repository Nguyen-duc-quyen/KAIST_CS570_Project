project: "CS570_AI_ML_Diffusions"
exp_name: "ddim-cifar10-unet"


# Hierarchical configuration
defaults:
  - dataset@train_set: cifar10_train
  - dataset@test_set: cifar10_test
  - noise_scheduler: ddim
  - model: unet_2d
  - _self_


# Training configuration
# For the first three configuration, please check if the config for model and dataloader match
mean: [0.4914, 0.4822, 0.4465]
std: [0.247, 0.243, 0.261]
input_shape: [3, 32, 32]


num_epochs: 500
log_interval: 1
save_checkpoint_interval: 50
val_interval: -1  # Validate after interval
devices: [0] # List of gpu indices. If the list is empty, run on CPU
use_tensorboard: False
use_wandb: True
precision: "32-true" # See Pytorch Lightning docs for more details
base_lr: 1e-4 # Base learning rate
batchsize: 128
num_workers: 16
samples_dir: "./samples"
checkpoint_dir: "./checkpoints"
checkpoint_path: "/home/quyennd/Data/KAIST_Courses/CS_570_AI_ML/CS_570_Latent_Consistency_Models/outputs/2025-05-02/ddim_cifar10_19-51-43/checkpoints/last.ckpt" # Path to the checkpoint file for resuming training


# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  lr: ${base_lr}
  weight_decay: 5e-5


# Loss function configuration
loss:
  _target_: torch.nn.MSELoss


# Learning rate scheduler
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ConstantLR
  factor: 1.0

# Timestep scheduler configuration
timestep_scheduler:
  _target_: timestep_samplers.samplers.UniformSampler


# Metrics
metrics:
  - _target_: torchmetrics.image.fid.FrechetInceptionDistance
    feature: 64
    normalize: True
    input_img_size: 32
  - _target_: torchmetrics.image.inception.InceptionScore
    splits: 10
    normalize: True


# Dataset configuration
train_set:
  batchsize: ${batchsize}
  num_workers: ${num_workers}
  drop_last: True
  shuffle: True

test_set:
  batchsize: ${batchsize}
  num_workers: ${num_workers}
  drop_last: False
  shuffle: False